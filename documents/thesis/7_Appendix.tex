% from https://tex.stackexchange.com/a/174628
\appendix
\addcontentsline{toc}{section}{Appendix}
\section*{Appendix}
\label{sec:appendix}

\renewcommand{\thesubsection}{\Alph{subsection}}

\subsection{Translation Table}
\label{sec:apdx-translation}

Below are some terms we used in this thesis and their corresponding terms as used in the source code of the Eio library and in our mechanization.

\begin{table}[ht]
    \begin{tabular}{l|l|l}
        Thesis                                 & Eio                                    & Mechanization           \\
                                                 \hline
        \ocamlin{Fiber.fork_promise}           & \ocamlin{Fiber.fork_promise}           & \ocamlin{fork_promise}  \\
        \ocamlin{Promise.await}                & \ocamlin{Promise.await}                & \ocamlin{await}         \\
        \ocamlin{Scheduler.run}                & \ocamlin{Sched.run}                    & \ocamlin{run}           \\
        \ocamlin{Domain_manager.new_scheduler} & \ocamlin{Domain_manager.run}           & \ocamlin{new_scheduler} \\
        \ocamlin{waker} function               & \ocamlin{enqueue}                      & \ocamlin{waker}         \\
        \ocamlin{register} function            & \ocamlin{f}                            & \ocamlin{register}      \\
        \emph{broadcast}                       & \ocamlin{Broadcast} \& \ocamlin{Cells} & \ocamlin{CQS}           \\
    \end{tabular}
\end{table}

\subsection{Adding Multithreading to \hh{}}
\label{sec:apdx-mt}

\ocf{} added not only effect handlers but also the ability to run OCaml code in parallel using multiple system-level threads, called \emph{Multicore OCaml}\footnote{While it was previously possible to run code in multiple threads using the \ocamlin{Thread} module, only one thread could execute OCaml code at a time, so it was not truly parallel.}.
\ocf{} adopts the name \emph{domain} for a parallel thread of execution, although in the following we prefer to use the term thread.
Each domain in \ocf{} corresponds to one system-level thread and the usual rules of multithreaded execution apply, i.e. domains are preemptively scheduled and have shared memory.
Eio defines a \emph{domain manager} to make use of multithreading by spawning a new thread and running a separate scheduler in it.
So while each Eio scheduler only runs fibers in a single thread, multiple threads can run separate schedulers and fibers can communicate with fibers running in separate threads.

Heaplang supports reasoning about multithreaded programs by implementing fork and join operations for threads and defining atomic steps in the operational semantics, which enables opening Iris \emph{shared invariants} for one such step.
In contrast, \hh{} did not define any multithreaded operational semantics, but as the language is based on heaplang it contained most of the required building blocks already.
In the following we explain which missing pieces we added to enable writing multithreaded programs in \hh{}.

\paragraph*{Atomic Variables in \ocf{}}
In the \ocf{} memory model, \emph{atomic variables} are needed in order to access shared memory without introducing data races.
Instead of modelling atomic variables in \hh{}, we treat all references as memory locations that are potentially shared with different threads, 
because in the standard formulation of multithreaded operational semantics in Iris all memory operations are sequentially consistent by construction. 
That means even normal load and store operations on locations are treated as synchronized accesses.
This seems to be the standard approach as it is the same in heaplang.

\subsubsection*{Extending the Operational Semantics}

To allow reasoning in \hazel{} about multithreaded programs we need a multithreaded operational semantics for \hh{}, as well as specifications for the new primitive operations \xfork{}, \xcmp{} (\emph{compare-and-exchange}) and \xfaa{} (\emph{fetch-and-add}).

The language interface of Iris provides a way to easily define a multithreaded semantics $\to_{mt}$ via a \emph{thread-pool}, 
provided one defines a \emph{thread-local} operational semantics $\to_t$.
The thread-pool is a list of expressions that represents threads running in parallel.
At each step, one expression is picked out of the pool at random and executed for one thread-local step.
Each thread-local step additionally returns a list of forked off threads, which are then added to the pool.
This is only relevant for the \xfork{} operation, as all other operations naturally don't fork off threads.

\[
\inferrule[MTStep]
{(e, \sigma) \to_{t} (e', \sigma', es')}
{(es_1 \mdpp e \mdpp es_2, \sigma) \to_{mt} (es_1 \mdpp e' \mdpp es_2 \mdpp es', \sigma')}
\]

We extend the existing thread-local operational semantics of \hh{} with the expressions \xfork{}, \xcmp{} and \xfaa{}, taking their definitions from heaplang.
Additionally, we need to prove specifications for the three operations.
\xcmp{} and \xfaa{} are standard and the same as heaplang, so we will not discuss them here.
The only interesting design decision in the case of \hh{} is how effects and the \xfork{} expression interact.
This design is guided by the fact that in \ocf{} effects never cross thread-boundaries.
An unhandled effect that propagates up to the top level is treated as an error and crashes the program.
As such we must impose the empty protocol on a newly forked off thread, independent on what protocol \(\Psi\) the main thread obeys.

\[
\inferrule[Step-Fork]
{\ewp{e}{\bot}{\top}}
{\ewp{fork\; e}{\Psi}{v.\; v = ()}}
\]

Using these primitive operations we can then build the standard $\mathit{CAS}$ (\emph{compare-and-set}), $spawn$, and $join$ operations on top and prove their specifications.
These constructions and specifications were also taken from heaplang.
Note that for \emph{spawn} we must also impose the empty protocol on $f$ as this expression will be forked off.

\begin{mathpar}
  \inferrule[Thread-Spawn]
  {\ewp{f\; ()}{\bot}{v.\; Q\; v}}
  {\ewp{thread\_spawn\; f}{\Psi}{j.\; joinHandle\; j\; Q}}
  %
  \and
  %
  \inferrule[Thread-Join]
  {joinHandle\; j\; Q}
  {\ewp{thread\_join\; j}{\Psi}{v.\; Q\; v}}
\end{mathpar}

This allows us to implement standard multithreaded programs which also use effect handlers.

\subsubsection*{Using Invariants in \hazel{}}

Invariants in Iris can be used to share resources between threads and are a crucial element to proving specifications of multithreaded programs.
They encapsulate a resource to be shared and can be opened for a single atomic step of execution.
During this step the resource can be taken out of the invariant and used in the proof but at the end of the step the invariant must be restored.

\hazel{} did already have the basic elements necessary to support using invariants.
It defined a ghost cell to hold the invariants and had an invariant access lemma which allows opening an invariant if the current expression is atomic.
So in order to use invariants we only had to provide proofs for which expressions are atomic.
We provided proofs for all primitive evaluation steps.
The proofs are the same for all steps and always consist of two parts.
First, we need to show that the expression is \coqin{head_atomic}, i.e. that it steps to a value in one step, which is true by definition of the operational semantics.
Then, we need to show that all subexpressions are values, which is true by construction for the expressions we show to be atomic.
%
\begin{minted}{coq}
Definition head_atomic (e : expr) : Prop :=
  ∀ σ e' σ' efs,
    head_step e σ e' σ' efs → is_Some (to_val e').
Definition sub_exprs_are_values (e : expr) :=
  ∀ K e', e = fill K e' → to_val e' = None → K = [].
Lemma ectx_language_atomic e :
  head_atomic e → sub_exprs_are_values e → Atomic e.
Proof. (* ... *) Qed.

Instance load_atomic v : Atomic (Load (Val v)). 
Proof. (* ... *) Qed.
Instance store_atomic v1 v2 : Atomic (Store (Val v1) (Val v2)). 
Proof. (* ... *) Qed.
Instance cmpxchg_atomic v1 v2 v3 : Atomic (CmpXchg (Val v1) (Val v2) (Val v3)). 
Proof. (* ... *) Qed.
\end{minted}
%
Since performing an effect starts a chain of evaluation steps to capture the current continuation, it is never atomic.
Therefore, invariants and effects do not interact in any interesting way.
